{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e73090e-0704-412e-b037-987f7cf3c32b",
   "metadata": {},
   "source": [
    "**CS-634: Data Mining - Final Project**  \n",
    "**Student Name:** Benyamin Plaksienko  \n",
    "**Instructor Name:** Yasser Abduallah  \n",
    "\n",
    "---\n",
    "\n",
    "**Project Title:**  \n",
    "Predicting Diabetes Using Supervised Data Mining (Classification) Binary\n",
    "Classification,Long Short-Term Memory, Gaussian Naive Bayes, and Random Forest Algorithm \n",
    "\n",
    "---\n",
    "**Note: this program requires the following prereqs**\n",
    "\n",
    "- **Python Version**: 3.8.20\n",
    "- **Conda Version**: 24.11.3\n",
    "- **Python Libraries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac24999b-ea9e-46a3-9e1b-e32da9bfdc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e93762-a50b-4286-9be4-861dfacefdca",
   "metadata": {},
   "source": [
    "# Diabetes Data Analysis\n",
    "\n",
    "This project uses the **Diabetes Dataset**, retrieved from [Kaggle](https://www.kaggle.com/datasets/mathchi/diabetes-data-set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e2d12-64d9-470b-8155-f6173add7e0b",
   "metadata": {},
   "source": [
    "# Classifier Performance Evaluation\n",
    "\n",
    "## Key Terminology\n",
    "\n",
    "- **True Positive (TP):** The number of positive examples correctly predicted by the model.\n",
    "- **True Negative (TN):** The number of negative examples correctly predicted by the model.\n",
    "- **False Positive (FP):** The number of negative examples wrongly predicted as positive by the model.\n",
    "- **False Negative (FN):** The number of positive examples wrongly predicted as negative by the model.\n",
    "\n",
    "Let \\( P \\) be the number of positive examples:  \n",
    "$$\n",
    "P = TP + FN\n",
    "$$\n",
    "Let \\( N \\) be the number of negative examples:  \n",
    "$$\n",
    "N = TN + FP\n",
    "$$\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### True Positive Rate (TPR) or Sensitivity\n",
    "The fraction of positive examples predicted correctly by the model:  \n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$\n",
    "\n",
    "### True Negative Rate (TNR) or Specificity\n",
    "The fraction of negative examples predicted correctly by the model:  \n",
    "$$\n",
    "TNR = \\frac{TN}{TN + FP} = \\frac{TN}{N}\n",
    "$$\n",
    "\n",
    "### False Positive Rate (FPR)\n",
    "The fraction of negative examples predicted as positive:  \n",
    "$$\n",
    "FPR = \\frac{FP}{TN + FP} = \\frac{FP}{N}\n",
    "$$\n",
    "\n",
    "### False Negative Rate (FNR)\n",
    "The fraction of positive examples predicted as negative:  \n",
    "$$\n",
    "FNR = \\frac{FN}{TP + FN} = \\frac{FN}{P}\n",
    "$$\n",
    "\n",
    "### Precision (p)\n",
    "The quality of the positive prediction:  \n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "### Recall (r) or Sensitivity\n",
    "The same as **True Positive Rate (TPR):**  \n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$\n",
    "\n",
    "### F1 Measure\n",
    "The harmonic mean of Precision and Recall:  \n",
    "$$\n",
    "F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "### Accuracy (Acc)\n",
    "The proportion of correctly predicted labels:  \n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + FP + FN + TN} = \\frac{TP + TN}{P + N}\n",
    "$$\n",
    "\n",
    "### Error Rate (Err)\n",
    "The proportion of incorrect predictions:  \n",
    "$$\n",
    "ErrorRate = \\frac{FP + FN}{TP + FP + FN + TN} = \\frac{FP + FN}{P + N}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Metrics\n",
    "\n",
    "### Balanced Accuracy (BACC)\n",
    "The average of **True Positive Rate (TPR)** and **True Negative Rate (TNR):**  \n",
    "$$\n",
    "Balanced\\_Accuracy = \\frac{TPR + TNR}{2}\n",
    "$$\n",
    "\n",
    "### True Skill Statistic (TSS)\n",
    "The difference between **True Positive Rate (TPR)** and **False Positive Rate (FPR):**  \n",
    "$$\n",
    "TSS = TPR - FPR\n",
    "$$\n",
    "\n",
    "### Heidke Skill Score (HSS)\n",
    "A measure of prediction over random prediction:  \n",
    "$$\n",
    "HSS = \\frac{2 \\times (TP \\times TN - FP \\times FN)}{(TP + FP) \\times (FN + TN) + (TP + FN) \\times (TN + FP)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66100b4-31d1-4816-8251-22a108f61655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    #Confusion matrix to get TP,TN,FP, and FN\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    #print(f'{len(cm)} x {len(cm[0])}') this was just to check shape\n",
    "    \n",
    "    #print(cm.shape)\n",
    "    TP, FN, FP, TN = cm.flatten() if cm.shape == (2,2) else (0,0,0,0)  \n",
    "\n",
    "    # True Positive Rate\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "    # True Negative Rate\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "    # False Positive Rate\n",
    "    FPR = FP / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "    # False Negative Rate\n",
    "    FNR = FN / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "    # Precision\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else 0 \n",
    "    # F1 Measure\n",
    "    F1 = (2 * Precision * TPR) / (Precision + TPR) if (Precision + TPR) != 0 else 0\n",
    "    # Accuracy\n",
    "    Accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) != 0 else 0\n",
    "    # Balanced Accuracy (BACC)\n",
    "    Balanced_Accuracy = (TPR + TNR) / 2  \n",
    "    # Error Rate\n",
    "    ErrorRate = (FP + FN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) != 0 else 0  \n",
    "    # True Skill Statistic\n",
    "    TSS = TPR - FPR  \n",
    "    # Heidke Skill Score\n",
    "    HSS = (2 * (TP * TN - FP * FN)) / ((TP + FP) * (FN + TN) + (TP + FN) * (TN + FP)) if (TP + FP) * (FN + TN) + (TP + FN) * (TN + FP) != 0 else 0 \n",
    "    #Total\n",
    "    T=TP+FN+TN+FP\n",
    "    #Total Positive\n",
    "    P = TP+FN\n",
    "    #Total Negative\n",
    "    N =TN+FP\n",
    "    \n",
    "    return TP, TN, FP, FN, FPR, FNR, TSS, HSS, Precision, F1, Accuracy, Balanced_Accuracy, ErrorRate, T , P , N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eceb6-e3ad-40ee-a563-8af14fb6900a",
   "metadata": {},
   "source": [
    "# **Diabetes Prediction: Machine Learning Model Comparison**\n",
    "\n",
    "## **Objective**\n",
    "This notebook compares three different machine learning models for classifying diabetes outcomes:\n",
    "\n",
    "- **Random Forest Classifier**\n",
    "- **Gaussian Naïve Bayes**\n",
    "- **Long Short-Term Memory (LSTM) Neural Network**\n",
    "\n",
    "## **Workflow**\n",
    "\n",
    "### **1. Data Preprocessing**\n",
    "- Load the dataset (`diabetes.csv`).\n",
    "- Separate features (X) and the target variable (y).\n",
    "- Check class distribution to assess data skewness.\n",
    "- Standardize features for better model performance.\n",
    "- Reshape data for LSTM (3D input format).\n",
    "\n",
    "### **2. Model Training & Evaluation**\n",
    "- Use **10-Fold Cross-Validation** for robust model evaluation.\n",
    "- Train and test each model using different dataset splits.\n",
    "\n",
    "### **3. Performance Metrics Calculation**\n",
    "- The function **`calculate_metrics(y_true, y_pred)`** is used to compute:\n",
    "  - True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)\n",
    "  - Accuracy, Precision, Recall, F1-score, False Positive Rate (FPR), False Negative Rate (FNR)\n",
    "  - True Skill Statistic (TSS), Heidke Skill Score (HSS), and Error Rate.\n",
    "- This function is **called three times** in the loop for each fold:\n",
    "  - **For Random Forest:** `metrics_rf = calculate_metrics(y_test, y_pred_rf)`\n",
    "  - **For Gaussian Naïve Bayes:** `metrics_gnb = calculate_metrics(y_test, y_pred_gnb)`\n",
    "  - **For LSTM:** `metrics_lstm = calculate_metrics(y_test, y_pred_lstm)`\n",
    "\n",
    "### **4. Results & Comparison**\n",
    "- Display results for each fold in a tabular format.\n",
    "- Compute **average metrics across all folds**.\n",
    "- Compare models to determine the best-performing approach.\n",
    "\n",
    "## **Conclusion**\n",
    "This notebook evaluates machine learning models for diabetes prediction, providing insights into their strengths and weaknesses. The final results help in selecting the most effective model based on accuracy and other performance metrics.\n",
    "\n",
    "# *key detail*\n",
    "\n",
    "Since my diabetes dataset does not have timestamps or sequential dependencies, the LSTM isn't truly leveraging its full potential for capturing temporal relationships. However, it still functions as a neural network, just in a slightly unconventional way it is treated as if there is 1 time step for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9645ac4a-0dd6-40bd-85b2-fa00f506d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome\n",
      "0    0.651042\n",
      "1    0.348958\n",
      "Name: proportion, dtype: float64\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "\n",
      "Metrics for Fold 1:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    1 RandomForest  43  13  17   4 0.566667 0.085106 0.348227 0.404115   0.716667 0.803738  0.727273           0.674113   0.272727 77 47 30\n",
      "    1   GaussianNB  43  17  13   4 0.433333 0.085106 0.481560 0.525135   0.767857 0.834951  0.779221           0.740780   0.220779 77 47 30\n",
      "    1         LSTM  45  15  15   2 0.500000 0.042553 0.457447 0.530864   0.750000 0.841121  0.779221           0.728723   0.220779 77 47 30\n",
      "\n",
      "Metrics for Fold 2:\n",
      " Fold    Algorithm  TP  TN  FP  FN  FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    2 RandomForest  45  16   9   7 0.36 0.134615 0.505385 0.516916   0.833333 0.849057  0.792208           0.752692   0.207792 77 52 25\n",
      "    2   GaussianNB  44  15  10   8 0.40 0.153846 0.446154 0.456334   0.814815 0.830189  0.766234           0.723077   0.233766 77 52 25\n",
      "    2         LSTM  45  13  12   7 0.48 0.134615 0.385385 0.410656   0.789474 0.825688  0.753247           0.692692   0.246753 77 52 25\n",
      "\n",
      "Metrics for Fold 3:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    3 RandomForest  48  13  11   5 0.458333 0.094340 0.447327 0.487575   0.813559 0.857143  0.792208           0.723664   0.207792 77 53 24\n",
      "    3   GaussianNB  46  11  13   7 0.541667 0.132075 0.326258 0.355613   0.779661 0.821429  0.740260           0.663129   0.259740 77 53 24\n",
      "    3         LSTM  48  11  13   5 0.541667 0.094340 0.363994 0.411922   0.786885 0.842105  0.766234           0.681997   0.233766 77 53 24\n",
      "\n",
      "Metrics for Fold 4:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    4 RandomForest  38  16   8  15 0.333333 0.283019 0.383648 0.361749   0.826087 0.767677  0.701299           0.691824   0.298701 77 53 24\n",
      "    4   GaussianNB  39  14  10  14 0.416667 0.264151 0.319182 0.307110   0.795918 0.764706  0.688312           0.659591   0.311688 77 53 24\n",
      "    4         LSTM  41  15   9  12 0.375000 0.226415 0.398585 0.386728   0.820000 0.796117  0.727273           0.699292   0.272727 77 53 24\n",
      "\n",
      "Metrics for Fold 5:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    5 RandomForest  47  19   4   7 0.173913 0.129630 0.696457 0.673676   0.921569 0.895238  0.857143           0.848229   0.142857 77 54 23\n",
      "    5   GaussianNB  46  17   6   8 0.260870 0.148148 0.590982 0.577498   0.884615 0.867925  0.818182           0.795491   0.181818 77 54 23\n",
      "    5         LSTM  47  17   6   7 0.260870 0.129630 0.609501 0.602228   0.886792 0.878505  0.831169           0.804750   0.168831 77 54 23\n",
      "\n",
      "Metrics for Fold 6:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    6 RandomForest  39  16  13   9 0.448276 0.187500 0.364224 0.376672   0.750000 0.780000  0.714286           0.682112   0.285714 77 48 29\n",
      "    6   GaussianNB  38  14  15  10 0.517241 0.208333 0.274425 0.286787   0.716981 0.752475  0.675325           0.637213   0.324675 77 48 29\n",
      "    6         LSTM  42  16  13   6 0.448276 0.125000 0.426724 0.456572   0.763636 0.815534  0.753247           0.713362   0.246753 77 48 29\n",
      "\n",
      "Metrics for Fold 7:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR  FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    7 RandomForest  42  16  11   8 0.407407 0.16 0.432593 0.445461   0.792453 0.815534  0.753247           0.716296   0.246753 77 50 27\n",
      "    7   GaussianNB  41  16  11   9 0.407407 0.18 0.412593 0.420377   0.788462 0.803922  0.740260           0.706296   0.259740 77 50 27\n",
      "    7         LSTM  45  15  12   5 0.444444 0.10 0.455556 0.493976   0.789474 0.841121  0.779221           0.727778   0.220779 77 50 27\n",
      "\n",
      "Metrics for Fold 8:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    8 RandomForest  48  16   7   6 0.304348 0.111111 0.584541 0.592170   0.872727 0.880734  0.831169           0.792271   0.168831 77 54 23\n",
      "    8   GaussianNB  46  17   6   8 0.260870 0.148148 0.590982 0.577498   0.884615 0.867925  0.818182           0.795491   0.181818 77 54 23\n",
      "    8         LSTM  47  16   7   7 0.304348 0.129630 0.566023 0.566023   0.870370 0.870370  0.818182           0.783011   0.181818 77 54 23\n",
      "\n",
      "Metrics for Fold 9:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    9 RandomForest  32  17  21   6 0.552632 0.157895 0.289474 0.313932   0.603774 0.703297  0.644737           0.644737   0.355263 76 38 38\n",
      "    9   GaussianNB  34  21  17   4 0.447368 0.105263 0.447368 0.475175   0.666667 0.764045  0.723684           0.723684   0.276316 76 38 38\n",
      "    9         LSTM  35  20  18   3 0.473684 0.078947 0.447368 0.485167   0.660377 0.769231  0.723684           0.723684   0.276316 76 38 38\n",
      "\n",
      "Metrics for Fold 10:\n",
      " Fold    Algorithm  TP  TN  FP  FN  FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "   10 RandomForest  44  18   7   7 0.28 0.137255 0.582745 0.582745   0.862745 0.862745  0.815789           0.791373   0.184211 76 51 25\n",
      "   10   GaussianNB  43  16   9   8 0.36 0.156863 0.483137 0.488308   0.826923 0.834951  0.776316           0.741569   0.223684 76 51 25\n",
      "   10         LSTM  45  19   6   6 0.24 0.117647 0.642353 0.642353   0.882353 0.882353  0.842105           0.821176   0.157895 76 51 25\n",
      "\n",
      "Metrics Across All Folds for Random Forest:\n",
      " Fold    Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    1 RandomForest  43  13  17   4 0.566667 0.085106 0.348227 0.404115   0.716667 0.803738  0.727273           0.674113   0.272727 77 47 30\n",
      "    2 RandomForest  45  16   9   7 0.360000 0.134615 0.505385 0.516916   0.833333 0.849057  0.792208           0.752692   0.207792 77 52 25\n",
      "    3 RandomForest  48  13  11   5 0.458333 0.094340 0.447327 0.487575   0.813559 0.857143  0.792208           0.723664   0.207792 77 53 24\n",
      "    4 RandomForest  38  16   8  15 0.333333 0.283019 0.383648 0.361749   0.826087 0.767677  0.701299           0.691824   0.298701 77 53 24\n",
      "    5 RandomForest  47  19   4   7 0.173913 0.129630 0.696457 0.673676   0.921569 0.895238  0.857143           0.848229   0.142857 77 54 23\n",
      "    6 RandomForest  39  16  13   9 0.448276 0.187500 0.364224 0.376672   0.750000 0.780000  0.714286           0.682112   0.285714 77 48 29\n",
      "    7 RandomForest  42  16  11   8 0.407407 0.160000 0.432593 0.445461   0.792453 0.815534  0.753247           0.716296   0.246753 77 50 27\n",
      "    8 RandomForest  48  16   7   6 0.304348 0.111111 0.584541 0.592170   0.872727 0.880734  0.831169           0.792271   0.168831 77 54 23\n",
      "    9 RandomForest  32  17  21   6 0.552632 0.157895 0.289474 0.313932   0.603774 0.703297  0.644737           0.644737   0.355263 76 38 38\n",
      "   10 RandomForest  44  18   7   7 0.280000 0.137255 0.582745 0.582745   0.862745 0.862745  0.815789           0.791373   0.184211 76 51 25\n",
      "\n",
      "Metrics Across All Folds for Gaussian Naive Bayes:\n",
      " Fold  Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    1 GaussianNB  43  17  13   4 0.433333 0.085106 0.481560 0.525135   0.767857 0.834951  0.779221           0.740780   0.220779 77 47 30\n",
      "    2 GaussianNB  44  15  10   8 0.400000 0.153846 0.446154 0.456334   0.814815 0.830189  0.766234           0.723077   0.233766 77 52 25\n",
      "    3 GaussianNB  46  11  13   7 0.541667 0.132075 0.326258 0.355613   0.779661 0.821429  0.740260           0.663129   0.259740 77 53 24\n",
      "    4 GaussianNB  39  14  10  14 0.416667 0.264151 0.319182 0.307110   0.795918 0.764706  0.688312           0.659591   0.311688 77 53 24\n",
      "    5 GaussianNB  46  17   6   8 0.260870 0.148148 0.590982 0.577498   0.884615 0.867925  0.818182           0.795491   0.181818 77 54 23\n",
      "    6 GaussianNB  38  14  15  10 0.517241 0.208333 0.274425 0.286787   0.716981 0.752475  0.675325           0.637213   0.324675 77 48 29\n",
      "    7 GaussianNB  41  16  11   9 0.407407 0.180000 0.412593 0.420377   0.788462 0.803922  0.740260           0.706296   0.259740 77 50 27\n",
      "    8 GaussianNB  46  17   6   8 0.260870 0.148148 0.590982 0.577498   0.884615 0.867925  0.818182           0.795491   0.181818 77 54 23\n",
      "    9 GaussianNB  34  21  17   4 0.447368 0.105263 0.447368 0.475175   0.666667 0.764045  0.723684           0.723684   0.276316 76 38 38\n",
      "   10 GaussianNB  43  16   9   8 0.360000 0.156863 0.483137 0.488308   0.826923 0.834951  0.776316           0.741569   0.223684 76 51 25\n",
      "\n",
      "Metrics Across All Folds for LSTM:\n",
      " Fold Algorithm  TP  TN  FP  FN      FPR      FNR      TSS      HSS  Precision       F1  Accuracy  Balanced_Accuracy  ErrorRate  T  P  N\n",
      "    1      LSTM  45  15  15   2 0.500000 0.042553 0.457447 0.530864   0.750000 0.841121  0.779221           0.728723   0.220779 77 47 30\n",
      "    2      LSTM  45  13  12   7 0.480000 0.134615 0.385385 0.410656   0.789474 0.825688  0.753247           0.692692   0.246753 77 52 25\n",
      "    3      LSTM  48  11  13   5 0.541667 0.094340 0.363994 0.411922   0.786885 0.842105  0.766234           0.681997   0.233766 77 53 24\n",
      "    4      LSTM  41  15   9  12 0.375000 0.226415 0.398585 0.386728   0.820000 0.796117  0.727273           0.699292   0.272727 77 53 24\n",
      "    5      LSTM  47  17   6   7 0.260870 0.129630 0.609501 0.602228   0.886792 0.878505  0.831169           0.804750   0.168831 77 54 23\n",
      "    6      LSTM  42  16  13   6 0.448276 0.125000 0.426724 0.456572   0.763636 0.815534  0.753247           0.713362   0.246753 77 48 29\n",
      "    7      LSTM  45  15  12   5 0.444444 0.100000 0.455556 0.493976   0.789474 0.841121  0.779221           0.727778   0.220779 77 50 27\n",
      "    8      LSTM  47  16   7   7 0.304348 0.129630 0.566023 0.566023   0.870370 0.870370  0.818182           0.783011   0.181818 77 54 23\n",
      "    9      LSTM  35  20  18   3 0.473684 0.078947 0.447368 0.485167   0.660377 0.769231  0.723684           0.723684   0.276316 76 38 38\n",
      "   10      LSTM  45  19   6   6 0.240000 0.117647 0.642353 0.642353   0.882353 0.882353  0.842105           0.821176   0.157895 76 51 25\n",
      "\n",
      "Average Metrics Across All Folds for Random Forest:\n",
      "TP                   42.600000\n",
      "TN                   16.000000\n",
      "FP                   10.800000\n",
      "FN                    7.400000\n",
      "FPR                   0.388491\n",
      "FNR                   0.148047\n",
      "TSS                   0.463462\n",
      "HSS                   0.475501\n",
      "Precision             0.799291\n",
      "F1                    0.821516\n",
      "Accuracy              0.762936\n",
      "Balanced_Accuracy     0.731731\n",
      "ErrorRate             0.237064\n",
      "T                    76.800000\n",
      "P                    50.000000\n",
      "N                    26.800000\n",
      "dtype: float64\n",
      "\n",
      "Average Metrics Across All Folds for Gaussian Naive Bayes:\n",
      "TP                   42.000000\n",
      "TN                   15.800000\n",
      "FP                   11.000000\n",
      "FN                    8.000000\n",
      "FPR                   0.404542\n",
      "FNR                   0.158193\n",
      "TSS                   0.437264\n",
      "HSS                   0.446983\n",
      "Precision             0.792651\n",
      "F1                    0.814252\n",
      "Accuracy              0.752597\n",
      "Balanced_Accuracy     0.718632\n",
      "ErrorRate             0.247403\n",
      "T                    76.800000\n",
      "P                    50.000000\n",
      "N                    26.800000\n",
      "dtype: float64\n",
      "\n",
      "Average Metrics Across All Folds for LSTM:\n",
      "TP                   44.000000\n",
      "TN                   15.700000\n",
      "FP                   11.100000\n",
      "FN                    6.000000\n",
      "FPR                   0.406829\n",
      "FNR                   0.117878\n",
      "TSS                   0.475293\n",
      "HSS                   0.498649\n",
      "Precision             0.799936\n",
      "F1                    0.836215\n",
      "Accuracy              0.777358\n",
      "Balanced_Accuracy     0.737647\n",
      "ErrorRate             0.222642\n",
      "T                    76.800000\n",
      "P                    50.000000\n",
      "N                    26.800000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "#Separate features (X) and target variable (y)\n",
    "#(Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age) are the features\n",
    "X = data.drop(columns=['Outcome'])\n",
    "\n",
    "y = data['Outcome']  \n",
    "#Show class distribution/ Data skewing for target outcomes (obviously the data is a bit skewed)\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "#Standardize the features (for better model performance)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Reshape data for LSTM: We need a 3D array (samples, time steps, features)\n",
    "X_scaled_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "#model setup\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=420)\n",
    "gnb_model = GaussianNB()\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  \n",
    "    model.add(LSTM(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "lstm_model = create_lstm_model((X_scaled_lstm.shape[1], X_scaled_lstm.shape[2]))\n",
    "\n",
    "#Set up 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=420)\n",
    "\n",
    "\n",
    "metrics_rf_list = []\n",
    "metrics_gnb_list = []\n",
    "metrics_lstm_list = []\n",
    "fold_dfs = []\n",
    "columns = [\"Fold\", \"Algorithm\", \"TP\", \"TN\", \"FP\", \"FN\", \"FPR\", \"FNR\", \"TSS\", \"HSS\", \n",
    "           \"Precision\", \"F1\", \"Accuracy\", \"Balanced_Accuracy\", \"ErrorRate\", \"T\", \"P\", \"N\"]\n",
    "\n",
    "#Perform KFold cross-validation and calculate metrics for each fold for all models\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_scaled), start=1):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    #Random Forest\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    metrics_rf = calculate_metrics(y_test, y_pred_rf)\n",
    "    metrics_entry_rf = [fold, \"RandomForest\", *metrics_rf]\n",
    "    metrics_rf_list.append(metrics_entry_rf)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    gnb_model.fit(X_train, y_train)\n",
    "    y_pred_gnb = gnb_model.predict(X_test)\n",
    "    metrics_gnb = calculate_metrics(y_test, y_pred_gnb)\n",
    "    metrics_entry_gnb = [fold, \"GaussianNB\", *metrics_gnb]\n",
    "    metrics_gnb_list.append(metrics_entry_gnb)\n",
    "    #------------------------------------------------------------------------------------\n",
    "    \n",
    "    #LSTM \n",
    "    X_train_lstm, X_test_lstm = X_scaled_lstm[train_index], X_scaled_lstm[test_index]#Prepare data for LSTM\n",
    "    lstm_model.fit(X_train_lstm, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    y_pred_lstm = (lstm_model.predict(X_test_lstm) > 0.5).astype(int).flatten()\n",
    "    metrics_lstm = calculate_metrics(y_test, y_pred_lstm)\n",
    "    metrics_entry_lstm = [fold, \"LSTM\", *metrics_lstm]\n",
    "    metrics_lstm_list.append(metrics_entry_lstm)\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    fold_df = pd.DataFrame([metrics_entry_rf, metrics_entry_gnb, metrics_entry_lstm], columns=columns)\n",
    "    fold_dfs.append(fold_df)\n",
    "\n",
    "#PRINTING tabular format listing all details for easier visualization for each fold and average\n",
    "for fold, fold_df in enumerate(fold_dfs, start=1):\n",
    "    print(f\"\\nMetrics for Fold {fold}:\")\n",
    "    print(fold_df.to_string(index=False))\n",
    "\n",
    "df_rf = pd.DataFrame(metrics_rf_list, columns=columns)\n",
    "df_gnb = pd.DataFrame(metrics_gnb_list, columns=columns)\n",
    "df_lstm = pd.DataFrame(metrics_lstm_list, columns=columns)\n",
    "\n",
    "print(\"\\nMetrics Across All Folds for Random Forest:\")\n",
    "print(df_rf.to_string(index=False))\n",
    "print(\"\\nMetrics Across All Folds for Gaussian Naive Bayes:\")\n",
    "print(df_gnb.to_string(index=False))\n",
    "print(\"\\nMetrics Across All Folds for LSTM:\")\n",
    "print(df_lstm.to_string(index=False))\n",
    "\n",
    "df_rf_no_fold = df_rf.drop(columns=['Fold'])\n",
    "df_gnb_no_fold = df_gnb.drop(columns=['Fold'])\n",
    "df_lstm_no_fold = df_lstm.drop(columns=['Fold'])\n",
    "average_metrics_rf = df_rf_no_fold.mean(numeric_only=True)\n",
    "average_metrics_gnb = df_gnb_no_fold.mean(numeric_only=True)\n",
    "average_metrics_lstm = df_lstm_no_fold.mean(numeric_only=True)\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Folds for Random Forest:\")\n",
    "print(average_metrics_rf)\n",
    "print(\"\\nAverage Metrics Across All Folds for Gaussian Naive Bayes:\")\n",
    "print(average_metrics_gnb)\n",
    "print(\"\\nAverage Metrics Across All Folds for LSTM:\")\n",
    "print(average_metrics_lstm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cea63-7d29-4523-9730-b13d06acf494",
   "metadata": {},
   "source": [
    "# **Which algorithm performs better and why?**\n",
    "\n",
    "To understand which algorithm performed better, we need to analyze the results.\n",
    "\n",
    "## LSTM (Long Short-Term Memory)\n",
    "\n",
    "- **Pros:**\n",
    "  - LSTM leads with the highest **F1 score** (0.8384), **Accuracy** (0.780), and **Balanced Accuracy** (0.7403). This suggests that LSTM provides the best balance between precision and recall, achieving the highest overall accuracy.\n",
    "  - It has the **lowest Error Rate** (0.2200), indicating that it is the most accurate model in terms of minimizing misclassifications.\n",
    "  - LSTM has the **lowest False Negative Rate (FNR)** (0.114), suggesting it is better at avoiding false negatives compared to other models. For instance, Random Forest has an FNR of 0.148, and GNB (Gaussian Naive Bayes) has an FNR of 0.158.\n",
    "  - It also has the **highest True Skill Statistic (TSS)** (0.4807) and **Heidke Skill Score (HSS)** (0.5047), reflecting the best overall model performance in terms of distinguishing between classes.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "- **Pros:**\n",
    "  - Random Forest has the **lowest False Positive Rate (FPR)** (0.388), meaning it is less likely to incorrectly classify a negative instance as positive.\n",
    "# **Conclusion**\n",
    "Based on the analysis, **LSTM** is the best-performing model overall, with superior metrics in **Accuracy**, **Balanced Accuracy**, and **Error Rate**. It excels at minimizing misclassifications and false negatives, as well as distinguishing between classes, making it the most reliable model. **Random Forest** performs well in terms of the **False Positive Rate** but falls short in other areas like **FNR** and **Accuracy**. **GNB**, while useful in certain cases, has the highest **FNR** and lower overall performance, placing it in the third position.\n",
    "# **Discussion**\n",
    "In general, I was expecting Random Forest to perform better than LSTM because my data isn't sequential, so it doesn't benefit from Long Short-Term Memory in that regard. Naive Bayes doesn't handle non-linear data well, and due to the high dimensionality of this dataset, I expected it to be the worst-performing algorithm. I believe the way I organized the LSTM, with 1 time step for each sample, is partially the reason it performs better than Random Forest. It kind of works like a Dense Neural Network, and due to the amount of data I'm inputting, it can outperform Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e60a1-8460-4187-81d7-02f651702b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
